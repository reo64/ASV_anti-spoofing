{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LFCC-GMM trainning baseline\n",
    "   #### number of training = 25,380\n",
    "    genuine 2,580 spoof 22,800\n",
    "   \n",
    "   #### number of development = 24,986\n",
    "    genuine 2,548 spoof 22,296"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GMMs(Gaussian Mixture Models) front-end are LFCCs and CQCCs.\n",
    "# My library\n",
    "from lfcc import *\n",
    "\n",
    "# Library for dataloader\n",
    "import os.path\n",
    "import glob\n",
    "\n",
    "# Library for LFCC-GMM\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from sklearn.externals import joblib\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Library for reading flac audio file\n",
    "import soundfile as sf\n",
    "#from scipy.io.wavfile import read\n",
    "\n",
    "# Library for pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torchvision import models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_formatter = \"{:.4f}\".format\n",
    "\n",
    "np.set_printoptions(formatter={'float_kind': float_formatter})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preproess, Dataset, Dataloader definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess(object):\n",
    "    \"\"\"\n",
    "    Preprocessing class for audio data\n",
    "    \n",
    "    Attributes:\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        \"\"\"\n",
    "        self.extractor = None\n",
    "        self.features = None\n",
    "        \n",
    "    def __call__(self, y, sr, feature, dynamic=True):\n",
    "        \"\"\"\n",
    "        Extract fetures with lfcc, mfcc, cqcc and other method\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        \"\"\"\n",
    "        if feature == 'LFCC':\n",
    "            self.extractor = LFCC(y, sr)\n",
    "            \n",
    "        elif feature == 'MFCC':\n",
    "            self.extractor = MFCC(y, sr)\n",
    "        \n",
    "        elif feature == 'CQCC':\n",
    "            self.extractor = CQCC(y, sr)\n",
    "        \n",
    "        self.features = self.extractor.extract_feature()\n",
    "        \n",
    "        return self.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess test\n",
    "process = Preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cqcc.py\t\t   lfcc_genuine.csv  lfcc_spoofed.csv  s1260057_report1\r\n",
      "GMMs_train.ipynb   lfcc_gmm.py\t     mfcc.py\t       utterance0.wav\r\n",
      "LA_T_1028533.flac  lfcc.py\t     __pycache__       utterance3.wav\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "[-41.1161 -0.4636 0.3527 0.3396 -0.0897 -0.0397 0.2201 -0.0644 -0.0815\n",
      " 0.4321 0.0985 0.1912]\n",
      "\n",
      "[-41.1636 -0.4733 0.1317 0.1415 -0.0173 0.4239 0.0381 0.2045 0.2349\n",
      " -0.0618 -0.0771 -0.0280]\n"
     ]
    }
   ],
   "source": [
    "y, sr = sf.read('LA_T_1028533.flac')\n",
    "\n",
    "print(sr*32//1000)\n",
    "\n",
    "ext = LFCC(y, sr)\n",
    "lfcc = ext.extract_feature().T\n",
    "\n",
    "print(lfcc[0])\n",
    "print()\n",
    "print(lfcc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/DB/Audio/English/ASVspoof2019/LA/ASVspoof2019_LA_train/flac/*.flac\n",
      "/DB/Audio/English/ASVspoof2019/LA/ASVspoof2019_LA_dev/flac/*.flac\n"
     ]
    }
   ],
   "source": [
    "def make_datapath_list(phase='train'):\n",
    "    \"\"\"\n",
    "    make a list containing a path to data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    phase: 'train' or 'dev' or 'eval'\n",
    "        specify whether data is for train or development or evaluation\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    path_list : list\n",
    "        return a list containing a path to data\n",
    "    \"\"\"\n",
    "    \n",
    "    root_path = \"/DB/Audio/English/ASVspoof2019/LA/\"\n",
    "    target_path = os.path.join(root_path+'ASVspoof2019_LA_'+phase+'/flac/*.flac')\n",
    "    print(target_path)\n",
    "    \n",
    "    path_list = []\n",
    "    \n",
    "    # Get a filepath to subdir by using glob module\n",
    "    for path in glob.glob(target_path):\n",
    "        path_list.append(path)\n",
    "    \n",
    "    return path_list\n",
    "\n",
    "# test\n",
    "train_list = make_datapath_list(phase='train')\n",
    "dev_list = make_datapath_list(phase='dev')\n",
    "\n",
    "#print(train_list)\n",
    "\n",
    "#print(dev_list)\n",
    "\n",
    "#print(len(train_list), len(dev_list))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename: LA_T_4693111, label: spoof\n",
      "12-dimentional vectors (149, 12)\n",
      "audiofile label:  spoof\n",
      "\n",
      "filename: LA_T_9729458, label: spoof\n",
      "12-dimentional vectors (155, 12)\n",
      "audiofile label:  spoof\n",
      "\n",
      "filename: LA_T_6008224, label: spoof\n",
      "12-dimentional vectors (152, 12)\n",
      "audiofile label:  spoof\n",
      "\n",
      "filename: LA_T_7196968, label: spoof\n",
      "12-dimentional vectors (139, 12)\n",
      "audiofile label:  spoof\n",
      "\n",
      "filename: LA_T_6911273, label: spoof\n",
      "12-dimentional vectors (196, 12)\n",
      "audiofile label:  spoof\n",
      "\n",
      "filename: LA_T_4450164, label: spoof\n",
      "12-dimentional vectors (345, 12)\n",
      "audiofile label:  spoof\n",
      "\n",
      "filename: LA_T_3662604, label: spoof\n",
      "12-dimentional vectors (215, 12)\n",
      "audiofile label:  spoof\n",
      "\n",
      "filename: LA_T_6713123, label: spoof\n",
      "12-dimentional vectors (143, 12)\n",
      "audiofile label:  spoof\n",
      "\n",
      "filename: LA_T_4504002, label: spoof\n",
      "12-dimentional vectors (149, 12)\n",
      "audiofile label:  spoof\n",
      "\n",
      "filename: LA_T_1771704, label: bonafide\n",
      "12-dimentional vectors (201, 12)\n",
      "audiofile label:  bonafide\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make dataloader\n",
    "class ASVspoofDataSet(data.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for ASVspoof2019, which derived from torch.utils.data.Dataset class\n",
    "    \n",
    "    Attributes:\n",
    "    --------------\n",
    "    file_list: list\n",
    "        list containing a path to data\n",
    "        \n",
    "    transform: object\n",
    "        instance of PreProcessor\n",
    "    \n",
    "    phase: str\n",
    "        'train' or 'dev' or 'eval'\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, file_list, label_list=None, preprocess=None, phase='train'):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        file_list: list\n",
    "            list of audio files to read\n",
    "        \n",
    "        label_list: list\n",
    "            list of labels('bonafide' or 'spoof'), which is changed to 0, 1\n",
    "        \n",
    "        transform: class PreProcess\n",
    "            instance of PreProcess to be used for pre-process to audio data\n",
    "        \n",
    "        phase: str\n",
    "            specify whether data is for training or development or evaluation('train' or 'dev' or 'eval')\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        self.phase = phase\n",
    "        self.preprocess = preprocess\n",
    "        self.root_path = '/DB/Audio/English/ASVspoof2019/LA/'\n",
    "        #self.file_path = None\n",
    "        self.file_list = file_list\n",
    "        self.label_path = None\n",
    "        self.label_list = label_list\n",
    "        \n",
    "        if self.phase == 'train':\n",
    "            self.label_path = os.path.join(self.root_path+'ASVspoof2019_LA_cm_protocols/')\n",
    "            self.label_list = []\n",
    "            with open(self.label_path+'ASVspoof2019.LA.cm.train.trn.txt', mode='r') as protocols:\n",
    "                for line in protocols:\n",
    "                    line = line.split() # read line by line\n",
    "                    filename, label = line[1], line[-1] # get filename and label from protocols file\n",
    "                    self.label_list.append((filename, label))\n",
    "                    \n",
    "        elif self.phase == 'dev':\n",
    "            self.label_path = None\n",
    "        else:\n",
    "            self.label_path = None\n",
    "        \n",
    "    def __len__(self): # this is needed to be overrided\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, index): # this is also needed to be overrided\n",
    "        \"\"\"\n",
    "        Get data and its label that was pre-processed\n",
    "        \"\"\"\n",
    "        \n",
    "        # load audio\n",
    "        speech_path = self.file_list[index]\n",
    "        speech, sr = sf.read(speech_path)\n",
    "        \n",
    "        # preprocessing and extract features\n",
    "        features = self.preprocess(y=speech, sr=sr, feature='LFCC') # preprocess to speech, not implemented yet\n",
    "        \n",
    "        label = None\n",
    "        speech_name = speech_path.split('/')[-1].rstrip('.flac')\n",
    "        \n",
    "        for fname, la in self.label_list:\n",
    "            #print(fname)\n",
    "            if fname == speech_name: # compare to speech_name with '==' annotation, check if they have same value.\n",
    "                label = la\n",
    "                print(\"filename: {}, label: {}\".format(fname, label))\n",
    "        \n",
    "        #print(\"sp name:\", speech_name)\n",
    "        return features, label\n",
    "    \n",
    "# test\n",
    "\n",
    "file_list = train_list\n",
    "\n",
    "asvspoof_train = ASVspoofDataSet(file_list=file_list, preprocess=process, phase='train')\n",
    "\n",
    "# get 10 files and its label\n",
    "iterations = 10\n",
    "\n",
    "for itr in range(iterations):\n",
    "    feature, label = asvspoof_train.__getitem__(itr)\n",
    "    print(\"12-dimentional vectors\", feature.T.shape)\n",
    "    print(\"audiofile label: \", label)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMMs training section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper parameters for training\n",
    "\n",
    "LFCCs:\n",
    "\n",
    "    window_len = 20ms\n",
    "    nfft = 512\n",
    "    # of filters = 20\n",
    "    0th-ceps = removed\n",
    "    dynamic features = delta, delta-delta included\n",
    "\n",
    "GMMs:\n",
    "\n",
    "    n_components = 512\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename: LA_T_6956794, label: spoof\n",
      "filename: LA_T_4814564, label: spoof\n",
      "filename: LA_T_5266968, label: spoof\n",
      "filename: LA_T_7393963, label: spoof\n",
      "filename: LA_T_2987671, label: bonafide\n",
      "filename: LA_T_9052152, label: spoof\n",
      "filename: LA_T_3617820, label: spoof\n",
      "filename: LA_T_2012289, label: bonafide\n",
      "filename: LA_T_6900974, label: spoof\n",
      "filename: LA_T_4080381, label: spoof\n",
      "filename: LA_T_7743702, label: spoof\n",
      "filename: LA_T_4974655, label: spoof\n",
      "filename: LA_T_2444482, label: spoof\n",
      "filename: LA_T_2403525, label: spoof\n",
      "filename: LA_T_8681938, label: spoof\n",
      "filename: LA_T_9183771, label: spoof\n",
      "filename: LA_T_9364074, label: spoof\n",
      "filename: LA_T_3510282, label: spoof\n",
      "filename: LA_T_8791775, label: spoof\n",
      "filename: LA_T_8101792, label: spoof\n",
      "filename: LA_T_2323946, label: spoof\n",
      "filename: LA_T_9089994, label: spoof\n",
      "filename: LA_T_1237039, label: spoof\n",
      "filename: LA_T_6564983, label: spoof\n",
      "filename: LA_T_8523979, label: spoof\n",
      "filename: LA_T_9279859, label: spoof\n",
      "filename: LA_T_4092741, label: spoof\n",
      "filename: LA_T_1402948, label: spoof\n",
      "filename: LA_T_1406603, label: bonafide\n",
      "filename: LA_T_3288573, label: bonafide\n",
      "filename: LA_T_3701743, label: spoof\n",
      "filename: LA_T_7384968, label: spoof\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Sizes of tensors must match except in dimension 0. Got 169 and 212 in dimension 2 at /opt/conda/conda-bld/pytorch_1579022030672/work/aten/src/TH/generic/THTensor.cpp:612",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-eadc413823cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mbatch_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_iterator\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# get first element\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# scalars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 169 and 212 in dimension 2 at /opt/conda/conda-bld/pytorch_1579022030672/work/aten/src/TH/generic/THTensor.cpp:612"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# instanciate DataLoader\n",
    "train_dataloader = data.DataLoader(asvspoof_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataloader = None #data.DataLoader()\n",
    "\n",
    "dataloader_dict = {\n",
    "    \"train\": train_dataloader,\n",
    "    \"val\": val_dataloader\n",
    "}\n",
    "\n",
    "batch_iterator = iter(dataloader_dict[\"train\"])\n",
    "inputs, labels = next(batch_iterator) # get first element\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_speech: 25380\n",
      "0.0 % complete\n",
      "filename: LA_T_4693111, label: spoof\n",
      "(149, 12)\n",
      "3.940110323089047e-05 % complete\n",
      "filename: LA_T_9729458, label: spoof\n",
      "(155, 12)\n",
      "7.880220646178094e-05 % complete\n",
      "filename: LA_T_6008224, label: spoof\n",
      "(152, 12)\n",
      "0.00011820330969267139 % complete\n",
      "filename: LA_T_7196968, label: spoof\n",
      "(139, 12)\n",
      "0.00015760441292356187 % complete\n",
      "filename: LA_T_6911273, label: spoof\n",
      "(196, 12)\n",
      "0.00019700551615445234 % complete\n",
      "filename: LA_T_4450164, label: spoof\n",
      "(345, 12)\n",
      "0.00023640661938534278 % complete\n",
      "filename: LA_T_3662604, label: spoof\n",
      "(215, 12)\n",
      "0.0002758077226162333 % complete\n",
      "filename: LA_T_6713123, label: spoof\n",
      "(143, 12)\n",
      "0.00031520882584712374 % complete\n",
      "filename: LA_T_4504002, label: spoof\n",
      "(149, 12)\n",
      "0.0003546099290780142 % complete\n",
      "filename: LA_T_1771704, label: bonafide\n",
      "(201, 12)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_path = '/DB/Audio/English/ASVspoof2019/LA/ASVspoof2019_LA_train/flac/*'\n",
    "dev_path = '/DB/Audio/English/ASVspoof2019/LA/ASVspoof2019_LA_dev/flac/*'\n",
    "\n",
    "speech_count = 0\n",
    "total = len(glob.glob(train_path))\n",
    "print(\"total_speech:\", total)\n",
    "\n",
    "genuine_df = pd.DataFrame()\n",
    "spoofed_df = pd.DataFrame()\n",
    "\n",
    "for itr in range(10):\n",
    "    \n",
    "    print('{} % complete'.format(speech_count/total))\n",
    "    \n",
    "    feature, label = asvspoof_train.__getitem__(itr)\n",
    "    #print(\"12-dimentional vectors\", feature.T.shape)\n",
    "    #print(\"audiofile label: \", label)\n",
    "    #print()\n",
    "    feature_df = pd.DataFrame(feature.T)\n",
    "    \n",
    "    print(feature_df.shape)\n",
    "    if label == 'bonafide':\n",
    "        genuine_df = genuine_df.append(feature_df, ignore_index=True)\n",
    "    else:\n",
    "        spoofed_df = spoofed_df.append(feature_df, ignore_index=True)\n",
    "    \n",
    "    speech_count += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201 1643\n"
     ]
    }
   ],
   "source": [
    "# check count\n",
    "print(len(genuine_df), len(spoofed_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "genuine_df.reset_index(drop=True).to_csv('./lfcc_genuine.csv', index=False)\n",
    "spoofed_df.reset_index(drop=True).to_csv('./lfcc_spoofed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-43.232835</td>\n",
       "      <td>0.773243</td>\n",
       "      <td>0.032616</td>\n",
       "      <td>0.320308</td>\n",
       "      <td>0.319756</td>\n",
       "      <td>0.349676</td>\n",
       "      <td>0.112323</td>\n",
       "      <td>0.140922</td>\n",
       "      <td>0.274338</td>\n",
       "      <td>0.078688</td>\n",
       "      <td>0.411345</td>\n",
       "      <td>-0.015514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-43.927803</td>\n",
       "      <td>0.460822</td>\n",
       "      <td>0.203643</td>\n",
       "      <td>0.238726</td>\n",
       "      <td>0.201849</td>\n",
       "      <td>0.418290</td>\n",
       "      <td>0.098958</td>\n",
       "      <td>0.193320</td>\n",
       "      <td>0.209587</td>\n",
       "      <td>0.291883</td>\n",
       "      <td>0.041371</td>\n",
       "      <td>-0.081493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-43.867943</td>\n",
       "      <td>0.522619</td>\n",
       "      <td>0.005994</td>\n",
       "      <td>0.257253</td>\n",
       "      <td>0.374527</td>\n",
       "      <td>0.407090</td>\n",
       "      <td>-0.013139</td>\n",
       "      <td>0.165098</td>\n",
       "      <td>0.280819</td>\n",
       "      <td>0.193640</td>\n",
       "      <td>0.145726</td>\n",
       "      <td>0.052421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-40.993793</td>\n",
       "      <td>2.217705</td>\n",
       "      <td>0.834241</td>\n",
       "      <td>-0.266068</td>\n",
       "      <td>0.601328</td>\n",
       "      <td>1.159979</td>\n",
       "      <td>0.612538</td>\n",
       "      <td>0.113617</td>\n",
       "      <td>0.434787</td>\n",
       "      <td>0.406434</td>\n",
       "      <td>0.053986</td>\n",
       "      <td>0.023301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-40.459587</td>\n",
       "      <td>3.130816</td>\n",
       "      <td>0.937090</td>\n",
       "      <td>-0.785774</td>\n",
       "      <td>0.258141</td>\n",
       "      <td>1.041662</td>\n",
       "      <td>0.683956</td>\n",
       "      <td>0.321447</td>\n",
       "      <td>0.759091</td>\n",
       "      <td>0.424619</td>\n",
       "      <td>-0.017425</td>\n",
       "      <td>0.123753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "0 -43.232835  0.773243  0.032616  0.320308  0.319756  0.349676  0.112323   \n",
       "1 -43.927803  0.460822  0.203643  0.238726  0.201849  0.418290  0.098958   \n",
       "2 -43.867943  0.522619  0.005994  0.257253  0.374527  0.407090 -0.013139   \n",
       "3 -40.993793  2.217705  0.834241 -0.266068  0.601328  1.159979  0.612538   \n",
       "4 -40.459587  3.130816  0.937090 -0.785774  0.258141  1.041662  0.683956   \n",
       "\n",
       "          7         8         9        10        11  \n",
       "0  0.140922  0.274338  0.078688  0.411345 -0.015514  \n",
       "1  0.193320  0.209587  0.291883  0.041371 -0.081493  \n",
       "2  0.165098  0.280819  0.193640  0.145726  0.052421  \n",
       "3  0.113617  0.434787  0.406434  0.053986  0.023301  \n",
       "4  0.321447  0.759091  0.424619 -0.017425  0.123753  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GMMs training\n",
    "\n",
    "# speaker embedding by using GMMs, where n_components = 512\n",
    "n_components = 512\n",
    "\n",
    "genuine_gmms = GaussianMixture(n_components=n_components, covariance_type='diag', max_iter=100, random_state=None)\n",
    "\n",
    "spoofed_gmms = GaussianMixture(n_components=n_components, covariance_type='diag', max_iter=100, random_state=None)\n",
    "\n",
    "Xg = pd.read_csv('./lfcc_genuine.csv')\n",
    "Xs = pd.read_csv('./lfcc_spoofed.csv')\n",
    "\n",
    "# Train the other parameters using the EM algorithm\n",
    "genuine_gmms.fit(Xg)\n",
    "spoofed_gmms.fit(Xs)\n",
    "\n",
    "Xg.head()\n",
    "Xs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_speech = len(glob.glob('the path to database of test data'))\n",
    "\n",
    "for i, speech in enumerate(sorted(glob.glob('the path to the database'))):\n",
    "    \n",
    "    print(i/n_speech,'percent completed')\n",
    "    \n",
    "    lfcc = LFCC(wavfile=speech).get_lfcc()\n",
    "\n",
    "    score = np.array([])\n",
    "    for lfcc_frame in lfcc.T:\n",
    "        loglh_genuine = genuine_gmms.score(lfcc_frame.reshape(1, -1))\n",
    "        loglh_spoofed = spoofed_gmms.score(lfcc_frame.reshape(1, -1))\n",
    "    \n",
    "    # compute mean\n",
    "    \n",
    "    # compute log-likelihood ratio\n",
    "    score = loglh_genuine - loglh_spoofed\n",
    "    \n",
    "    # store score to file\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
