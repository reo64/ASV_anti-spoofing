{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LCNN(Light Convolutional Neural Network)\n",
    "Front-end are LFCC, CQCC.<br>LCNN is adopted to perform back-end.\n",
    "\n",
    "### Aim\n",
    ">1. Edit pytorch dataset Done\n",
    ">2. Code LCNN architecture\n",
    ">3. Build LCNN model\n",
    ">4. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library for dataloader\n",
    "import os.path\n",
    "import glob\n",
    "from hdf5storage import loadmat\n",
    "\n",
    "# Feature extraction\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Library for pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "\n",
    "# Configuration\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set fixed random seed\n",
    "# Setting random seeds for reproducibility.\n",
    "seed = 120\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic=True # CUDA determinism \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_formatter = \"{:.4f}\".format\n",
    "\n",
    "np.set_printoptions(formatter={'float_kind': float_formatter})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/DB/Audio/English/ASVspoof2019/LA/ASVspoof2019_LA_train/flac/*\n",
      "/home/s1260057/workspace/GT/ASV_anti-spoofing/datasets/dev/LFCC/*\n",
      "25380 24844\n"
     ]
    }
   ],
   "source": [
    "def make_datapath_list(phase, feature_type):\n",
    "    \"\"\"\n",
    "    make a list containing a path to data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    phase: 'train' or 'dev' or 'eval'\n",
    "        specify whether data is for train or development or evaluation\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    path_list : list\n",
    "        return a list containing a path to data\n",
    "    \"\"\"\n",
    "    \n",
    "    if feature_type in ['CQT','LFCC','CQCC']:\n",
    "        root_path = \"/home/s1260057/workspace/GT/ASV_anti-spoofing/datasets/\"\n",
    "        target_path = os.path.join(root_path, phase, feature_type, '*')\n",
    "    \n",
    "    elif feature_type in ['SPEC','FFT']:\n",
    "        root_path = \"/DB/Audio/English/ASVspoof2019/LA/\"\n",
    "        target_path = os.path.join(root_path, 'ASVspoof2019_LA_'+phase+'/flac/*')\n",
    "        \n",
    "    else:\n",
    "        print('[Error: None of feature_types were matched.]')\n",
    "        raise AttributeError\n",
    "    \n",
    "    print(target_path)\n",
    "    \n",
    "    path_list = []\n",
    "    \n",
    "    # Get a filepath to subdir by using glob module\n",
    "    for path in sorted(glob.glob(target_path)):\n",
    "        path_list.append(path)\n",
    "    \n",
    "    return path_list\n",
    "\n",
    "# test\n",
    "train_list = make_datapath_list(phase='train', feature_type='FFT')\n",
    "dev_list = make_datapath_list(phase='dev', feature_type='LFCC')\n",
    "\n",
    "print(len(train_list), len(dev_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess(object):\n",
    "    \"\"\"\n",
    "    Preprocessing class for audio data\n",
    "    \n",
    "    Attributes:\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, shape):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        \"\"\"\n",
    "        self.shape = shape\n",
    "        \n",
    "    def __call__(self, d):\n",
    "        \"\"\"\n",
    "        Extract fetures with lfcc, mfcc, cqcc and other method\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        \"\"\"\n",
    "        n = self.shape[0] - d.shape[0]\n",
    "        # Cropping or truncation process might be inserted here, return fixed length feature matrix\n",
    "        if n <= 0:\n",
    "            x = self.padding(d, n)\n",
    "        else:\n",
    "            x = self.truncate(d, n)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def padding(self, x, n):\n",
    "        # Pad x to be n matrix\n",
    "        return x\n",
    "    \n",
    "    def truncate(self, x, n):\n",
    "        # Truncate x to be n matrix\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(object):\n",
    "    \"\"\"\n",
    "    FeatureExtractor class for audio data\n",
    "    \n",
    "    Attributes:\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, feature_type):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        \"\"\"\n",
    "        self.extractor = None\n",
    "        self.feature_type = feature_type\n",
    "        \n",
    "    def __call__(self, y, sr, dynamic=True):\n",
    "        \"\"\"\n",
    "        Extract fetures such as fft, spectrogram and other methods\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        \"\"\"\n",
    "        if self.feature_type == 'LFCC':\n",
    "            self.extractor = LFCC(y, sr)\n",
    "            \n",
    "        elif self.feature_type == 'MFCC':\n",
    "            self.extractor = MFCC(y, sr)\n",
    "        \n",
    "        elif self.feature_type == 'CQCC':\n",
    "            self.extractor = CQCC(y, sr)\n",
    "        \n",
    "        elif self.feature_type == 'FFT':\n",
    "            self.extractor = FFT(y, sr)\n",
    "        \n",
    "        elif self.feature_type == 'SPEC':\n",
    "            self.extractor = SPEC(y, sr)\n",
    "            \n",
    "        else:\n",
    "            print('Wrong feature extraction method specified')\n",
    "            raise AttributeError\n",
    "        \n",
    "        if dynamic:\n",
    "            features = self.extractor.extract_feature(delta=True)\n",
    "        else:\n",
    "            features = self.extractor.extract_feature()\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/DB/Audio/English/ASVspoof2019/LA/ASVspoof2019_LA_eval/flac/*\n",
      "71933\n",
      "60 vectors torch.Size([50888])\n",
      "audiofile label:  ('LA_0044', 'A10', 'spoof')\n",
      "\n",
      "60 vectors torch.Size([32986])\n",
      "audiofile label:  ('LA_0001', 'A15', 'spoof')\n",
      "\n",
      "60 vectors torch.Size([23846])\n",
      "audiofile label:  ('LA_0023', 'A11', 'spoof')\n",
      "\n",
      "60 vectors torch.Size([66346])\n",
      "audiofile label:  ('LA_0043', 'A09', 'spoof')\n",
      "\n",
      "60 vectors torch.Size([35489])\n",
      "audiofile label:  ('LA_0014', 'A14', 'spoof')\n",
      "\n",
      "60 vectors torch.Size([60312])\n",
      "audiofile label:  ('LA_0011', 'A11', 'spoof')\n",
      "\n",
      "60 vectors torch.Size([40884])\n",
      "audiofile label:  ('LA_0026', 'A17', 'spoof')\n",
      "\n",
      "60 vectors torch.Size([62487])\n",
      "audiofile label:  ('LA_0038', 'A10', 'spoof')\n",
      "\n",
      "60 vectors torch.Size([55968])\n",
      "audiofile label:  ('LA_0023', 'A12', 'spoof')\n",
      "\n",
      "60 vectors torch.Size([39633])\n",
      "audiofile label:  ('LA_0029', 'A09', 'spoof')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make dataloader\n",
    "class ASVspoofDataSet(data.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for ASVspoof2019, which derived from torch.utils.data.Dataset class\n",
    "    \n",
    "    Attributes:\n",
    "    --------------\n",
    "    file_list: list\n",
    "        list containing a path to data\n",
    "        \n",
    "    transform: object\n",
    "        instance of PreProcessor\n",
    "    \n",
    "    phase: str\n",
    "        'train' or 'dev' or 'eval'\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, file_list, phase, feature_type, preprocess=None, extractor=None, detailed_label=False):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        file_list: list\n",
    "            list of audio files to read\n",
    "        \n",
    "        label_list: list\n",
    "            list of labels('bonafide' or 'spoof'), which is changed to 0, 1\n",
    "        \n",
    "        transform: class PreProcess\n",
    "            instance of PreProcess to be used for pre-process to audio data\n",
    "        \n",
    "        phase: str\n",
    "            specify whether data is for training or development or evaluation('train' or 'dev' or 'eval')\n",
    "            \n",
    "        \"\"\"\n",
    "        self.file_list = file_list\n",
    "        self.phase = phase\n",
    "        self.feature_type = feature_type\n",
    "        self.preprocess = preprocess\n",
    "        self.feature_extract = extractor\n",
    "        self.detailed_label = detailed_label\n",
    "        \n",
    "        if self.detailed_label:\n",
    "            \n",
    "            self.root_path = '/DB/Audio/English/ASVspoof2019/LA/ASVspoof2019_LA_cm_protocols/'\n",
    "            \n",
    "            if self.phase == 'train':\n",
    "                self.label_path = os.path.join(self.root_path, 'ASVspoof2019.LA.cm.train.trn.txt')\n",
    "                self.label_list = []\n",
    "                with open(self.label_path, mode='r') as protocols:\n",
    "                    for line in protocols:\n",
    "                        line = line.split() # read line by line\n",
    "                        filename, label = line[1], line[-1] # get filename and label from protocols file\n",
    "                        self.label_list.append((filename, label))\n",
    "            \n",
    "            elif self.phase == 'dev':\n",
    "                self.label_path = os.path.join(self.root_path, 'ASVspoof2019.LA.cm.dev.trl.txt')\n",
    "                self.label_list = []\n",
    "                with open(self.label_path, mode='r') as protocols:\n",
    "                    for line in protocols:\n",
    "                        line = line.split() # read line by line\n",
    "                        filename, label = line[1], (line[0], line[3], line[-1]) # get items from protocols file\n",
    "                        self.label_list.append((filename, label))\n",
    "            \n",
    "            elif self.phase == 'eval':\n",
    "                self.label_path = os.path.join(self.root_path, 'ASVspoof2019.LA.cm.eval.trl.txt')\n",
    "                self.label_list = []\n",
    "                with open(self.label_path, mode='r') as protocols:\n",
    "                    for line in protocols:\n",
    "                        line = line.split()\n",
    "                        # Extract speaker_id, system_id, key\n",
    "                        filename, label = line[1], (line[0], line[3], line[-1])\n",
    "                        # Make a pair of filename and label\n",
    "                        self.label_list.append((filename, label))\n",
    "            else:\n",
    "                print(\"Unsupported phase specified. You must pass either phase='train' or 'dev' or 'eval'\")\n",
    "        \n",
    "    def __len__(self): # this is needed to be overrided\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, index): # this is also needed to be overrided\n",
    "        \"\"\"\n",
    "        Get preprocessed data and its label\n",
    "        \"\"\"\n",
    "        if self.feature_type in ['CQT','LFCC','CQCC']:\n",
    "            # load feature matrix\n",
    "            #print( type(loadmat(self.file_list[index])['x_fea']) ) => ndarray\n",
    "            features = loadmat(self.file_list[index])['x_fea']\n",
    "            label = self.file_list[index].split('_')[-1].rstrip('.mat')\n",
    "            \n",
    "        elif self.feature_type in ['SPEC','FFT']:\n",
    "            # load audio\n",
    "            speech_path = self.file_list[index]\n",
    "            speech, sr = sf.read(speech_path)\n",
    "            \n",
    "            ###\n",
    "            #return speech, sr\n",
    "            ###\n",
    "            if self.feature_extract:\n",
    "                features = self.feature_extract(y=speech, sr=sr)\n",
    "            else:\n",
    "                features = speech\n",
    "            \n",
    "            speech_name = speech_path.split('/')[-1].rstrip('.flac')\n",
    "            label = None\n",
    "            \n",
    "            for fname, key in self.label_list:\n",
    "                if fname == speech_name: # compare to speech_name with '==' annotation, check if they have same value.\n",
    "                    label = key\n",
    "                    #print(\"filename: {}, label: {}\".format(fname, label))\n",
    "            if label is None:\n",
    "                #print('[debug print] Not-labeled filename:', speech_name)\n",
    "                return None, None\n",
    "\n",
    "        else:\n",
    "            print('[Error: Unsupported feature_type]')\n",
    "            raise AttributeError\n",
    "            \n",
    "        # preprocessing and extract features\n",
    "        if self.preprocess:\n",
    "            features = self.preprocess(d=features)\n",
    "        #print(type(features))\n",
    "        tensor = torch.from_numpy(features).float()\n",
    "        #print(type(tensor))\n",
    "        \n",
    "        return tensor, label\n",
    "\n",
    "# Test\n",
    "\n",
    "phase = 'eval'\n",
    "\n",
    "feature_type = 'FFT'\n",
    "\n",
    "eval_list = make_datapath_list(phase=phase, feature_type=feature_type)\n",
    "print(len(eval_list))\n",
    "\n",
    "process = Preprocess(shape=(864, 400, 3))\n",
    "\n",
    "extractor = FeatureExtractor(feature_type=feature_type)\n",
    "\n",
    "asvspoof_eval = ASVspoofDataSet(file_list=eval_list, phase=phase,\n",
    "        feature_type=feature_type, preprocess=process,\n",
    "        extractor=None, detailed_label=True)\n",
    "\n",
    "# Get 10 files and their label\n",
    "iterations = 10\n",
    "\n",
    "for itr in range(iterations):\n",
    "    #print(asvspoof_train.file_list[itr])\n",
    "    feature, label = asvspoof_eval.__getitem__(itr)\n",
    "    print(\"60 vectors\", feature.T.shape)\n",
    "    print(\"audiofile label: \", label)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71933\n",
      "71237\n"
     ]
    }
   ],
   "source": [
    "print(len(asvspoof_eval.file_list))\n",
    "print(len(asvspoof_eval.label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal as signal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x, fs = sf.read('LA_T_1028533.flac')\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.arange(0, len(x))/fs\n",
    "\n",
    "plt.plot(xx, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = np.abs(np.fft.fft(x, n=1724)[:1724//2])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1724/16000*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "320/16000*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx, tx, Sxx = signal.spectrogram(x, fs, window=np.hamming(1724), nperseg=1724, noverlap=1724*0.0081, nfft=1724)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(221)\n",
    "image = plt.pcolormesh(tx, fx, np.log10(Sxx), shading='gouraud')\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.ylabel('Frequency [Hz]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sxx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f1, t1, Sxx1 = signal.spectrogram(x, fs, window=np.hamming(320), nperseg=320, noverlap=320//2, nfft=512, mode='magnitude')\n",
    "\n",
    "f2, t2, Sxx2 = signal.spectrogram(x, fs, window=np.hamming(320), nperseg=320, noverlap=320//2, nfft=512, mode='magnitude')\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(221)\n",
    "image = plt.pcolormesh(t1, f1, np.log10(Sxx1), shading='gouraud')\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.pcolormesh(t2, f2, np.log10(Sxx2)**2, shading='gouraud')\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x)/16000)\n",
    "print(Sxx1.shape)\n",
    "print(Sxx2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = librosa.core.stft(x, win_length=320, hop_length=320//2, n_fft=512, window=np.hamming(320), center=False)\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "\n",
    "iterations = 8\n",
    "for itr in range(iterations):\n",
    "    \n",
    "    x, sr = asvspoof_train.__getitem__(itr)\n",
    "    \n",
    "    f1, t1, Sxx1 = signal.spectrogram(x, sr, window=win, nperseg=320, noverlap=512//2, nfft=512, mode='magnitude')\n",
    "\n",
    "    plt.subplot(2, 4, itr+1)\n",
    "    plt.pcolormesh(t1, f1, np.log10(Sxx1), shading='gouraud')\n",
    "    plt.xlabel('Time [sec]')\n",
    "    plt.ylabel('Frequency [Hz]')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import librosa.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, sr = asvspoof_train.__getitem__(0)\n",
    "\n",
    "D = librosa.amplitude_to_db(np.abs(librosa.core.stft(x, win_length=320, hop_length=160, n_fft=512, window=np.hamming(320), center=False)), ref=np.max)\n",
    "\n",
    "librosa.display.specshow(D, sr=sr, hop_length=160, x_axis='time', y_axis='log')\n",
    "\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Log-frequency power spectrogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/DB/Audio/English/ASVspoof2019/LA/ASVspoof2019_LA_train/flac/*\n",
      "25380\n",
      "25380\n",
      "time_avg 3.4258241627265567\n",
      "time_var 2.0131270544304867\n",
      "10439 211007\n",
      "\n",
      "/DB/Audio/English/ASVspoof2019/LA/ASVspoof2019_LA_dev/flac/*\n",
      "24986\n",
      "24844\n",
      "time_avg 3.4781134705562713\n",
      "time_var 2.1265527199077794\n",
      "11122 185508\n",
      "\n",
      "/DB/Audio/English/ASVspoof2019/LA/ASVspoof2019_LA_eval/flac/*\n",
      "71933\n",
      "71237\n",
      "time_avg 3.1076934475764\n",
      "time_var 2.192651186467914\n",
      "7519 208409\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feature_type = 'FFT'\n",
    "\n",
    "process = Preprocess(shape=(864, 400, 3))\n",
    "\n",
    "extractor = FeatureExtractor(feature_type=feature_type)\n",
    "\n",
    "phase = ['train', 'dev', 'eval']\n",
    "\n",
    "sr = 16000\n",
    "\n",
    "for p in phase:\n",
    "    \n",
    "    dlist = make_datapath_list(phase=p, feature_type=feature_type)\n",
    "    \n",
    "    print(len(dlist))\n",
    "    \n",
    "    dataset = ASVspoofDataSet(file_list=dlist, phase=p, feature_type=feature_type, preprocess=process, extractor=None, detailed_label=True)\n",
    "    \n",
    "    x_total = 0\n",
    "    x_min, x_max = np.inf, -np.inf\n",
    "    llist = np.array([])\n",
    "    \n",
    "    for itr in range(len(dlist)):\n",
    "        x, label = dataset.__getitem__(itr)\n",
    "        \n",
    "        if label is None:\n",
    "            continue\n",
    "        \n",
    "        x_total += len(x)\n",
    "        llist = np.append(llist, len(x)/sr)\n",
    "        x_min = min(x_min, len(x))\n",
    "        x_max = max(x_max, len(x))\n",
    "    \n",
    "    print(len(llist))\n",
    "    \n",
    "    x_avg = x_total / len(llist) / sr\n",
    "    x_var = sum( (llist - x_avg)**2 ) / len(llist)\n",
    "    print('time_avg', x_avg)\n",
    "    print('time_var', x_var)\n",
    "    print(x_min, x_max)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Max-Feature-Map layer\n",
    "class MFM(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, type=1):\n",
    "        \n",
    "        super(MFM, self).__init__()\n",
    "        \n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        if type == 1:\n",
    "            self.filter = nn.Conv2d(in_channels, 2*out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        else:\n",
    "            self.filter = nn.Linear(in_channels, 2*out_channels)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.filter(x)\n",
    "        out = torch.split(x, self.out_channels, 1)\n",
    "        return torch.max(out[0], out[1])\n",
    "\n",
    "### end of class MFM(Max-Feature-Map activation)\n",
    "\n",
    "class Group(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "    \n",
    "        super(Group, self).__init__()\n",
    "        self.conv_a = MFM(in_channels=in_channels, out_channels=in_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv = MFM(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_a(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "### End of class Group ###\n",
    "\n",
    "class FC(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, out_size):\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(input_size, out_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class LCNN_4layers(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(LCNN_4layers, self).__init__()\n",
    "        \n",
    "        self.CNN = nn.Sequential(\n",
    "            MFM(in_channels=1, out_channels=32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True),\n",
    "            \n",
    "            MFM(in_channels=32, out_channels=48, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True),\n",
    "            \n",
    "            MFM(in_channels=48, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True),\n",
    "            \n",
    "            MFM(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True),\n",
    "        )\n",
    "        \n",
    "        self.FC1 = nn.Linear(54*25*32, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.MFM_FC1 = nn.Linear(512, 256)\n",
    "        self.FC2 = nn.Linear(54*25*32, 256)\n",
    "        #self.MFM_FC1 = MFM(in_channels=512, out_channels=256)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.CNN(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        #x = x.view(-1, self.num_flat_features(x))\n",
    "        x = self.FC1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.MFM_FC1(x)\n",
    "        return torch.sigmoid(x)\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "    def debug(self, x):\n",
    "        print(x.shape, type(x))\n",
    "### End of class LCNN_4layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LCNN_4layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1.5279, -0.5306, -0.8125,  ..., -0.0663, -0.2144, -0.8342],\n",
      "          [-1.2053, -1.6711, -1.5218,  ...,  0.9953, -0.0234, -1.1249],\n",
      "          [-1.5653, -1.6728, -0.5363,  ..., -0.3048, -1.2572,  0.9384],\n",
      "          ...,\n",
      "          [-1.2192, -1.4453,  0.5569,  ..., -0.2578, -3.0906,  0.8226],\n",
      "          [ 1.4315, -0.2452,  0.0921,  ...,  0.0855,  0.4544,  1.2625],\n",
      "          [-2.4970, -0.5660, -1.8089,  ..., -1.7885,  1.0715,  1.5377]]]])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(32, 1, 864, 400)\n",
    "print(input[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 288])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(32, 1, 5, 5)\n",
    "m = nn.Sequential(\n",
    "    nn.Conv2d(1, 32, 5, 1, 1),\n",
    "    nn.Flatten()\n",
    ")\n",
    "output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
